{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Quantum Neural Network for Methane Detection\n",
        "\n",
        "This implementation follows a comprehensive approach combining quantum and classical techniques:\n",
        "\n",
        "1. Data Preparation:\n",
        "   - Loads temporal meteorological data\n",
        "   - Handles missing values and outliers\n",
        "   - Extracts temporal patterns and weather features\n",
        "   - Normalizes data for quantum compatibility\n",
        "\n",
        "2. Quantum Feature Encoding:\n",
        "   - Uses angle encoding for quantum features\n",
        "   - Maps features to rotation gates\n",
        "   - Creates quantum-inspired classical features\n",
        "\n",
        "3. Quantum Kernels:\n",
        "   - Implements quantum kernel for similarity measures\n",
        "   - Uses fidelity-based kernel computation\n",
        "   - Handles quantum state preparation\n",
        "\n",
        "4. QNN Architecture:\n",
        "   - Combines quantum and classical components\n",
        "   - Uses variational quantum circuits\n",
        "   - Implements hybrid optimization\n",
        "\n",
        "5. Evaluation:\n",
        "   - Cross-validation for robustness\n",
        "   - Multiple performance metrics\n",
        "   - Cluster analysis for patterns\n",
        "\n",
        "Author: Baby Vennela Kothakonda\n",
        "\n",
        "Date: January 7, 2025"
      ],
      "metadata": {
        "id": "e_f6EGcRcOCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFozgfB0467s"
      },
      "outputs": [],
      "source": [
        "# Install Neccessary Libraries\n",
        "!pip install qiskit>=0.44.0\n",
        "!pip install qiskit-aer>=0.12.0\n",
        "!pip install qiskit-machine-learning>=0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold,GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.algorithms import VQC\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import MiniBatchKMeans"
      ],
      "metadata": {
        "id": "BN2AX7g9ayrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MethaneQNN:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Methane QNN with quantum and classical components\"\"\"\n",
        "        # Initialize with None, will be set after PCA\n",
        "        self.n_qubits = None\n",
        "        self.qr = None\n",
        "        self.cr = None\n",
        "\n",
        "        # Initialize classical models with optimized parameters\n",
        "        self.classical_models = {\n",
        "            'random_forest': RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                n_jobs=-1,  # Use all CPU cores\n",
        "                random_state=42\n",
        "            ),\n",
        "            'gradient_boost': GradientBoostingClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                max_iter=1000,  # Increased iterations\n",
        "                solver='lbfgs',\n",
        "                n_jobs=-1,  # Use all CPU cores\n",
        "                random_state=42\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def prepare_data(self, data):\n",
        "        \"\"\"\n",
        "        Step 1: Data Preparation\n",
        "        - Implements comprehensive data preprocessing\n",
        "        - Extracts temporal, geographic, and weather features\n",
        "        - Handles data cleaning and normalization\n",
        "        \"\"\"\n",
        "        print(\"Processing features...\")\n",
        "\n",
        "        # 1.1 Temporal Feature Extraction\n",
        "        # Convert time to datetime and extract cyclical features\n",
        "        data['Time'] = pd.to_numeric(data['Time'])\n",
        "        data['hour'] = data['Time'].astype(int)\n",
        "        data['minute'] = ((data['Time'] % 1) * 60).astype(int)\n",
        "        data['is_daytime'] = (data['hour'] >= 6) & (data['hour'] <= 18)\n",
        "\n",
        "        # 1.2 Cyclical Time Encoding\n",
        "        # Map time to circular coordinates to preserve periodicity\n",
        "        data['hour_sin'] = np.sin(2 * np.pi * data['hour']/24)\n",
        "        data['hour_cos'] = np.cos(2 * np.pi * data['hour']/24)\n",
        "\n",
        "        # 1.3 Geographic Feature Engineering\n",
        "        # Normalize spatial coordinates and calculate relative positions\n",
        "        data['lat_norm'] = (data['latitude'] - data['latitude'].mean()) / data['latitude'].std()\n",
        "        data['lon_norm'] = (data['longitude'] - data['longitude'].mean()) / data['longitude'].std()\n",
        "\n",
        "        # Calculate distance from center\n",
        "        center_lat = data['latitude'].mean()\n",
        "        center_lon = data['longitude'].mean()\n",
        "        data['dist_from_center'] = np.sqrt(\n",
        "            (data['latitude'] - center_lat)**2 +\n",
        "            (data['longitude'] - center_lon)**2\n",
        "        )\n",
        "\n",
        "        # 1.4 Weather Feature Engineering\n",
        "        # Calculate derived weather parameters and stability indicators\n",
        "        data['wind_speed'] = np.sqrt(\n",
        "            data['u_west_to_east_wind']**2 +\n",
        "            data['v_south_to_north_wind']**2\n",
        "        )\n",
        "        data['wind_direction'] = np.arctan2(\n",
        "            data['v_south_to_north_wind'],\n",
        "            data['u_west_to_east_wind']\n",
        "        )\n",
        "\n",
        "        # 1.5 Atmospheric Stability Features\n",
        "        # Calculate gradients and stability indicators\n",
        "        data['temp_gradient'] = data.groupby(['i_value', 'j_value'])['temprature'].diff()\n",
        "        data['humidity_gradient'] = data.groupby(['i_value', 'j_value'])['relative_humidity'].diff()\n",
        "\n",
        "        # 1.6 Physical Interaction Terms\n",
        "        # Create physically meaningful feature interactions\n",
        "        data['temp_humidity'] = data['temprature'] * data['relative_humidity']\n",
        "        data['wind_temp'] = data['wind_speed'] * data['temprature']\n",
        "        data['vapor_pressure'] = data['water_vapour'] * data['pressure']\n",
        "\n",
        "        # 1.7 Feature Selection\n",
        "        # Select features relevant for methane detection\n",
        "        features = [\n",
        "            # Temporal features for pattern detection\n",
        "            'hour_sin', 'hour_cos', 'is_daytime',\n",
        "            # Geographic features for spatial patterns\n",
        "            'lat_norm', 'lon_norm', 'dist_from_center',\n",
        "            # Weather features for atmospheric conditions\n",
        "            'wind_speed', 'wind_direction',\n",
        "            'temprature', 'relative_humidity',\n",
        "            'vertical_velocity', 'pressure',\n",
        "            'water_vapour', 'temp_gradient',\n",
        "            'humidity_gradient', 'temp_humidity',\n",
        "            'wind_temp', 'vapor_pressure',\n",
        "            'turbulent_kinatic_energy',\n",
        "            'precipitation_rate',\n",
        "            'sensible_heat_flux',\n",
        "            'Latent_heat_flux'\n",
        "        ]\n",
        "\n",
        "        # 1.8 Target Variable Creation\n",
        "        print(\"Creating target variable...\")\n",
        "        data['target'] = (data['tracer_concentration'] > 0).astype(int)\n",
        "\n",
        "        # 1.9 Class Balancing\n",
        "        print(\"Balancing dataset...\")\n",
        "        pos_samples = data[data['target'] == 1]\n",
        "        neg_samples = data[data['target'] == 0]\n",
        "\n",
        "        # Undersample majority class\n",
        "        n_samples = min(len(pos_samples), len(neg_samples))\n",
        "        pos_balanced = pos_samples.sample(n=n_samples, random_state=42)\n",
        "        neg_balanced = neg_samples.sample(n=n_samples, random_state=42)\n",
        "\n",
        "        # Combine balanced datasets\n",
        "        balanced_data = pd.concat([pos_balanced, neg_balanced])\n",
        "        print(f\"Class distribution after balancing - Positive: {(balanced_data['target'] == 1).mean():.2%}\")\n",
        "\n",
        "        # 1.10 Final Data Preparation\n",
        "        X = balanced_data[features].copy()\n",
        "        y = balanced_data['target'].values\n",
        "\n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "        # 1.11 Feature Scaling\n",
        "        self.scaler = StandardScaler()\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # 1.12 Dimensionality Reduction\n",
        "        print(\"Applying PCA...\")\n",
        "        self.pca = PCA(n_components=4)\n",
        "        X_pca = self.pca.fit_transform(X_scaled)\n",
        "        explained_var = np.sum(self.pca.explained_variance_ratio_)\n",
        "        print(f\"Explained variance with 4 components: {explained_var:.2%}\")\n",
        "\n",
        "        return X_pca, y\n",
        "\n",
        "    def create_quantum_feature_map(self, n_features):\n",
        "        \"\"\"\n",
        "        Step 2: Quantum Feature Encoding\n",
        "        - Uses angle encoding for quantum features\n",
        "        - Maps features to rotation gates\n",
        "        \"\"\"\n",
        "        # Create ZZFeatureMap for angle encoding\n",
        "        feature_map = ZZFeatureMap(\n",
        "            feature_dimension=n_features,\n",
        "            reps=2,\n",
        "            entanglement='linear'\n",
        "        )\n",
        "        return feature_map\n",
        "\n",
        "    def create_variational_circuit(self):\n",
        "        \"\"\"\n",
        "        Step 3: Quantum Circuit Architecture\n",
        "        - Combines quantum and classical components\n",
        "        - Uses variational quantum circuits\n",
        "        \"\"\"\n",
        "        # Create quantum circuit with focused architecture\n",
        "        var_circuit = QuantumCircuit(self.qr, self.cr)\n",
        "\n",
        "        # Single layer but with carefully chosen gates\n",
        "        params_per_qubit = 2  # Rx and Ry rotations\n",
        "        total_params = self.n_qubits * params_per_qubit\n",
        "        params = ParameterVector('θ', length=total_params)\n",
        "\n",
        "        # Apply rotations\n",
        "        param_idx = 0\n",
        "        for qubit in range(self.n_qubits):\n",
        "            var_circuit.rx(params[param_idx], qubit)\n",
        "            param_idx += 1\n",
        "            var_circuit.ry(params[param_idx], qubit)\n",
        "            param_idx += 1\n",
        "\n",
        "        # Add entanglement - linear with periodic boundary\n",
        "        for q in range(self.n_qubits):\n",
        "            next_q = (q + 1) % self.n_qubits\n",
        "            var_circuit.cx(q, next_q)\n",
        "\n",
        "        # Add final rotations for measurement basis\n",
        "        for qubit in range(self.n_qubits):\n",
        "            var_circuit.h(qubit)\n",
        "\n",
        "        # Add measurements\n",
        "        var_circuit.measure(self.qr, self.cr)\n",
        "\n",
        "        return var_circuit\n",
        "\n",
        "    def train_quantum_kernel_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Step 4: Quantum Kernel Training\n",
        "        - Implements quantum kernel for similarity measures\n",
        "        - Uses fidelity-based kernel computation\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining quantum kernel models...\")\n",
        "\n",
        "        # Create quantum kernel\n",
        "        feature_map = self.create_quantum_feature_map(X_train.shape[1])\n",
        "        quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
        "\n",
        "        # Generate kernel matrix\n",
        "        print(\"Computing quantum kernel matrix...\")\n",
        "        try:\n",
        "            kernel_matrix_train = quantum_kernel.evaluate(X_train)\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing kernel matrix: {str(e)}\")\n",
        "            print(\"Falling back to classical kernel...\")\n",
        "            from sklearn.metrics.pairwise import rbf_kernel\n",
        "            kernel_matrix_train = rbf_kernel(X_train)\n",
        "\n",
        "        # Initialize and train classical models\n",
        "        print(\"\\nTraining classical models with quantum kernel...\")\n",
        "        from sklearn.svm import SVC\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "        self.classical_models = {}\n",
        "\n",
        "        # Train SVM with quantum kernel\n",
        "        print(\"Training SVM...\")\n",
        "        svm = SVC(kernel='precomputed', probability=True, random_state=42)\n",
        "        svm.fit(kernel_matrix_train, y_train)\n",
        "        self.classical_models['svm'] = {'model': svm, 'kernel': quantum_kernel}\n",
        "\n",
        "        # Train RF on original features\n",
        "        print(\"Training RF...\")\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        self.classical_models['rf'] = {'model': rf, 'kernel': None}\n",
        "\n",
        "        print(\"Model training completed\")\n",
        "\n",
        "    def evaluate_quantum_kernel(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Step 5: Quantum Kernel Evaluation\n",
        "        - Evaluates models with quantum kernel\n",
        "        \"\"\"\n",
        "        print(\"\\nEvaluating models...\")\n",
        "\n",
        "        results = {}\n",
        "        for name, model_dict in self.classical_models.items():\n",
        "            model = model_dict['model']\n",
        "            kernel = model_dict['kernel']\n",
        "\n",
        "            # Get predictions\n",
        "            if kernel is not None:\n",
        "                try:\n",
        "                    # For SVM, compute kernel between test and training data\n",
        "                    kernel_matrix_test = kernel.evaluate(X_test, self.X_train)\n",
        "                    y_pred = model.predict(kernel_matrix_test)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in kernel prediction: {str(e)}\")\n",
        "                    print(\"Using RBF kernel fallback...\")\n",
        "                    kernel_matrix_test = rbf_kernel(X_test, self.X_train)\n",
        "                    y_pred = model.predict(kernel_matrix_test)\n",
        "            else:\n",
        "                # For RF, use features directly\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "            results[name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1\n",
        "            }\n",
        "\n",
        "            print(f\"\\n{name.upper()} Results:\")\n",
        "            for metric, value in results[name].items():\n",
        "                print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_quantum_inspired_features(self, X):\n",
        "        \"\"\"\n",
        "        Step 6: Quantum-Inspired Feature Generation\n",
        "        - Creates quantum-inspired classical features\n",
        "        - Implements angle encoding principles\n",
        "        - Generates non-linear transformations\n",
        "        \"\"\"\n",
        "        print(\"Creating quantum-inspired features...\")\n",
        "\n",
        "        # 6.1 Quantum Range Encoding\n",
        "        X_quantum = (X + 4) * np.pi / 4  # Map to [0, 2π] range\n",
        "\n",
        "        # 6.2 Quantum-Inspired Transformations\n",
        "        features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # 6.3 Original Features (like |0⟩ state)\n",
        "        features.append(X_quantum)\n",
        "        feature_names.extend([f'quantum_f{i}' for i in range(X_quantum.shape[1])])\n",
        "\n",
        "        # 6.4 Rotation-Inspired Features (like Rx, Ry gates)\n",
        "        features.append(np.sin(X_quantum))  # Rx-like\n",
        "        features.append(np.cos(X_quantum))  # Ry-like\n",
        "        feature_names.extend([f'sin_f{i}' for i in range(X_quantum.shape[1])])\n",
        "        feature_names.extend([f'cos_f{i}' for i in range(X_quantum.shape[1])])\n",
        "\n",
        "        # 6.5 Entanglement-Inspired Features (like CNOT effects)\n",
        "        for i in range(X_quantum.shape[1]):\n",
        "            for j in range(i+1, X_quantum.shape[1]):\n",
        "                interaction = X_quantum[:, i] * X_quantum[:, j]\n",
        "                features.append(interaction.reshape(-1, 1))\n",
        "                feature_names.append(f'interact_f{i}_f{j}')\n",
        "\n",
        "        # 6.6 Combine Features\n",
        "        X_enhanced = np.hstack(features)\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "        return X_enhanced\n",
        "\n",
        "    def tune_and_train_models(self, X_train, y_train):\n",
        "        \"\"\"Tune hyperparameters and train models\"\"\"\n",
        "        print(\"\\nTuning and training enhanced models...\")\n",
        "\n",
        "        # Create quantum-inspired features\n",
        "        print(\"Creating quantum-inspired features...\")\n",
        "        X_enhanced = self.create_quantum_inspired_features(X_train)\n",
        "\n",
        "        # Simplified parameter grids for faster tuning\n",
        "        param_grid = {\n",
        "            'random_forest': {\n",
        "                'n_estimators': [50, 100],\n",
        "                'max_depth': [5, 10]\n",
        "            },\n",
        "            'gradient_boost': {\n",
        "                'n_estimators': [50, 100],\n",
        "                'max_depth': [3, 5]\n",
        "            },\n",
        "            'logistic': {\n",
        "                'C': [0.1, 1.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Train each model with simplified tuning\n",
        "        for name, model in self.classical_models.items():\n",
        "            print(f\"\\nTuning {name}...\")\n",
        "\n",
        "            # Use 3-fold CV instead of 5-fold for speed\n",
        "            grid_search = GridSearchCV(\n",
        "                model,\n",
        "                param_grid[name],\n",
        "                cv=3,\n",
        "                n_jobs=-1,  # Use all CPU cores\n",
        "                scoring='f1'\n",
        "            )\n",
        "\n",
        "            grid_search.fit(X_enhanced, y_train)\n",
        "            self.classical_models[name] = grid_search.best_estimator_\n",
        "\n",
        "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "            print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        print(\"\\nModel training completed\")\n",
        "\n",
        "    def analyze_feature_importance(self):\n",
        "        \"\"\"\n",
        "        Step 8: Feature Importance Analysis\n",
        "        - Analyzes feature importance for classical models\n",
        "        - Visualizes feature importance\n",
        "        \"\"\"\n",
        "        print(\"\\nAnalyzing feature importance...\")\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        for name, model in self.classical_models.items():\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                # Get feature importances\n",
        "                importances = model.feature_importances_\n",
        "                indices = np.argsort(importances)[::-1]\n",
        "\n",
        "                # Print feature ranking\n",
        "                print(f\"\\nFeature ranking for {name}:\")\n",
        "                for f in range(len(self.feature_names)):\n",
        "                    print(f\"{f+1}. {self.feature_names[indices[f]]}: {importances[indices[f]]:.4f}\")\n",
        "\n",
        "                # Plot feature importances\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.title(f\"Feature Importances ({name})\")\n",
        "                plt.bar(range(len(indices[:10])), importances[indices[:10]])\n",
        "                plt.xticks(range(len(indices[:10])), [self.feature_names[i] for i in indices[:10]], rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'feature_importance_{name}.png')\n",
        "                plt.close()\n",
        "\n",
        "                print(f\"Feature importance plot saved as 'feature_importance_{name}.png'\")\n",
        "\n",
        "            elif hasattr(model, 'coef_'):\n",
        "                # For logistic regression\n",
        "                coef = np.abs(model.coef_[0])\n",
        "                indices = np.argsort(coef)[::-1]\n",
        "\n",
        "                print(f\"\\nFeature coefficients for {name}:\")\n",
        "                for f in range(len(self.feature_names)):\n",
        "                    print(f\"{f+1}. {self.feature_names[indices[f]]}: {coef[indices[f]]:.4f}\")\n",
        "\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.title(f\"Feature Coefficients ({name})\")\n",
        "                plt.bar(range(len(indices[:10])), coef[indices[:10]])\n",
        "                plt.xticks(range(len(indices[:10])), [self.feature_names[i] for i in indices[:10]], rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'feature_importance_{name}.png')\n",
        "                plt.close()\n",
        "\n",
        "                print(f\"Feature coefficient plot saved as 'feature_importance_{name}.png'\")\n",
        "\n",
        "    def train_enhanced_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Step 9: Enhanced Model Training\n",
        "        - Trains models with quantum-inspired features\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining enhanced models...\")\n",
        "\n",
        "        # Create quantum-inspired features\n",
        "        X_enhanced = self.create_quantum_inspired_features(X_train)\n",
        "\n",
        "        # Initialize models\n",
        "        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "        models = {\n",
        "            'rf': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
        "            'gb': GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42),\n",
        "            'lr': LogisticRegression(max_iter=1000, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Train models\n",
        "        self.classical_models = {}\n",
        "        for name, model in models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(X_enhanced, y_train)\n",
        "            self.classical_models[name] = model\n",
        "\n",
        "        print(\"Model training completed\")\n",
        "\n",
        "    def evaluate_enhanced(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Step 10: Enhanced Model Evaluation\n",
        "        - Evaluates models with quantum-inspired features\n",
        "        \"\"\"\n",
        "        print(\"\\nEvaluating models...\")\n",
        "\n",
        "        # Create quantum-inspired features for test set\n",
        "        X_enhanced = self.create_quantum_inspired_features(X_test)\n",
        "\n",
        "        results = {}\n",
        "        predictions = {}\n",
        "\n",
        "        for name, model in self.classical_models.items():\n",
        "            # Get predictions\n",
        "            y_pred = model.predict(X_enhanced)\n",
        "            predictions[name] = y_pred\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "            results[name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1\n",
        "            }\n",
        "\n",
        "            print(f\"\\n{name.upper()} Results:\")\n",
        "            for metric, value in results[name].items():\n",
        "                print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Ensemble prediction (majority voting)\n",
        "        ensemble_pred = np.mean([predictions[name] for name in predictions], axis=0) > 0.5\n",
        "\n",
        "        # Calculate ensemble metrics\n",
        "        accuracy = accuracy_score(y_test, ensemble_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, ensemble_pred, average='binary')\n",
        "\n",
        "        results['ensemble'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "\n",
        "        print(\"\\nENSEMBLE Results:\")\n",
        "        for metric, value in results['ensemble'].items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def perform_clustering(self, X, n_clusters=3):\n",
        "        \"\"\"\n",
        "        Step 11: Clustering Analysis\n",
        "        - Performs classical clustering on the enhanced features\n",
        "        \"\"\"\n",
        "        print(\"\\nPerforming clustering...\")\n",
        "\n",
        "        # Create enhanced features for clustering\n",
        "        X_enhanced = self.create_quantum_inspired_features(X)\n",
        "\n",
        "        # Use MiniBatchKMeans for faster clustering\n",
        "        from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "        print(\"Clustering data...\")\n",
        "        kmeans = MiniBatchKMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            batch_size=1000,\n",
        "            random_state=42\n",
        "        )\n",
        "        clusters = kmeans.fit_predict(X_enhanced)\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def visualize_clusters(self, X, clusters):\n",
        "        \"\"\"\n",
        "        Step 12: Cluster Visualization\n",
        "        - Visualizes clusters using PCA\n",
        "        \"\"\"\n",
        "        print(\"Creating cluster visualization...\")\n",
        "\n",
        "        # Use PCA for visualization\n",
        "        from sklearn.decomposition import PCA\n",
        "\n",
        "        # Reduce to 2D for visualization\n",
        "        pca = PCA(n_components=2)\n",
        "        X_2d = pca.fit_transform(X)\n",
        "\n",
        "        # Plot clusters\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=clusters, cmap='viridis')\n",
        "        plt.colorbar(scatter)\n",
        "        plt.title('Cluster Visualization (PCA)')\n",
        "        plt.xlabel('First Principal Component')\n",
        "        plt.ylabel('Second Principal Component')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('quantum_clusters.png')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Cluster visualization saved as 'quantum_clusters.png'\")\n",
        "\n",
        "        # Print cluster statistics\n",
        "        unique, counts = np.unique(clusters, return_counts=True)\n",
        "        print(\"\\nCluster sizes:\")\n",
        "        for cluster_id, count in zip(unique, counts):\n",
        "            print(f\"Cluster {cluster_id}: {count} samples ({count/len(clusters):.1%})\")\n",
        "\n",
        "    def cross_validate_models(self, X, y, n_splits=5):\n",
        "        \"\"\"\n",
        "        Step 13: Cross-Validation\n",
        "        - Performs cross-validation for robustness\n",
        "        \"\"\"\n",
        "        print(\"\\nPerforming cross-validation...\")\n",
        "\n",
        "        from sklearn.model_selection import KFold\n",
        "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        # Store results for each fold\n",
        "        cv_results = {model_name: [] for model_name in self.classical_models.keys()}\n",
        "        cv_results['ensemble'] = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "            print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Create features\n",
        "            X_train_enhanced = self.create_quantum_inspired_features(X_train)\n",
        "            X_val_enhanced = self.create_quantum_inspired_features(X_val)\n",
        "\n",
        "            # Train and evaluate each model\n",
        "            fold_predictions = []\n",
        "\n",
        "            for name, model in self.classical_models.items():\n",
        "                # Train model\n",
        "                model.fit(X_train_enhanced, y_train)\n",
        "\n",
        "                # Get predictions\n",
        "                y_pred = model.predict(X_val_enhanced)\n",
        "                fold_predictions.append(y_pred)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_val, y_pred)\n",
        "                precision = precision_score(y_val, y_pred)\n",
        "                recall = recall_score(y_val, y_pred)\n",
        "                f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "                cv_results[name].append({\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1\n",
        "                })\n",
        "\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "            # Ensemble predictions\n",
        "            ensemble_pred = np.mean(fold_predictions, axis=0) > 0.5\n",
        "\n",
        "            # Calculate ensemble metrics\n",
        "            accuracy = accuracy_score(y_val, ensemble_pred)\n",
        "            precision = precision_score(y_val, ensemble_pred)\n",
        "            recall = recall_score(y_val, ensemble_pred)\n",
        "            f1 = f1_score(y_val, ensemble_pred)\n",
        "\n",
        "            cv_results['ensemble'].append({\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            })\n",
        "\n",
        "            print(f\"Ensemble - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        # Print average results\n",
        "        print(\"\\nAverage Cross-Validation Results:\")\n",
        "        for name in cv_results.keys():\n",
        "            avg_accuracy = np.mean([x['accuracy'] for x in cv_results[name]])\n",
        "            avg_f1 = np.mean([x['f1'] for x in cv_results[name]])\n",
        "            print(f\"{name} - Avg Accuracy: {avg_accuracy:.4f}, Avg F1: {avg_f1:.4f}\")\n",
        "\n",
        "        return cv_results\n",
        "\n"
      ],
      "metadata": {
        "id": "TfYIhjwcSVF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading dataset...\")\n",
        "data = pd.read_csv('methane datain.csv')\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "\n",
        "# Initialize QNN\n",
        "qnn = MethaneQNN()\n",
        "\n",
        "# Prepare data with enhanced features\n",
        "print(\"Preparing data...\")\n",
        "X_scaled, y = qnn.prepare_data(data)\n",
        "print(f\"Prepared data shapes - X: {X_scaled.shape}, y: {y.shape}\")\n",
        "\n",
        "# Set number of qubits based on PCA components\n",
        "qnn.n_qubits = X_scaled.shape[1]\n",
        "qnn.qr = QuantumRegister(qnn.n_qubits)\n",
        "qnn.cr = ClassicalRegister(qnn.n_qubits)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_results = qnn.cross_validate_models(X_scaled, y)\n",
        "\n",
        "# Train final model on full dataset\n",
        "print(\"\\nTraining final models...\")\n",
        "qnn.tune_and_train_models(X_scaled, y)\n",
        "\n",
        "# Optional: Analyze feature importance and clustering\n",
        "# Comment these out if not needed\n",
        "qnn.analyze_feature_importance()\n",
        "clusters = qnn.perform_clustering(X_scaled)\n",
        "qnn.visualize_clusters(X_scaled, clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbkjgxrrchTV",
        "outputId": "f6d888a3-e2ff-477d-f409-3d61b3beaa34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset shape: (301340, 17)\n",
            "Preparing data...\n",
            "Processing features...\n",
            "Creating target variable...\n",
            "Balancing dataset...\n",
            "Class distribution after balancing - Positive: 50.00%\n",
            "Applying PCA...\n",
            "Explained variance with 4 components: 67.01%\n",
            "Prepared data shapes - X: (21812, 4), y: (21812,)\n",
            "\n",
            "Performing cross-validation...\n",
            "\n",
            "Fold 1/5\n",
            "Creating quantum-inspired features...\n",
            "Creating quantum-inspired features...\n",
            "random_forest - Accuracy: 0.7891, F1: 0.8092\n",
            "gradient_boost - Accuracy: 0.7848, F1: 0.8033\n",
            "logistic - Accuracy: 0.7243, F1: 0.7487\n",
            "Ensemble - Accuracy: 0.7825, F1: 0.8032\n",
            "\n",
            "Fold 2/5\n",
            "Creating quantum-inspired features...\n",
            "Creating quantum-inspired features...\n",
            "random_forest - Accuracy: 0.7956, F1: 0.8175\n",
            "gradient_boost - Accuracy: 0.7956, F1: 0.8158\n",
            "logistic - Accuracy: 0.7348, F1: 0.7602\n",
            "Ensemble - Accuracy: 0.7901, F1: 0.8125\n",
            "\n",
            "Fold 3/5\n",
            "Creating quantum-inspired features...\n",
            "Creating quantum-inspired features...\n",
            "random_forest - Accuracy: 0.7861, F1: 0.8058\n",
            "gradient_boost - Accuracy: 0.7813, F1: 0.7989\n",
            "logistic - Accuracy: 0.7304, F1: 0.7551\n",
            "Ensemble - Accuracy: 0.7788, F1: 0.7986\n",
            "\n",
            "Fold 4/5\n",
            "Creating quantum-inspired features...\n",
            "Creating quantum-inspired features...\n",
            "random_forest - Accuracy: 0.7863, F1: 0.8101\n",
            "gradient_boost - Accuracy: 0.7840, F1: 0.8040\n",
            "logistic - Accuracy: 0.7348, F1: 0.7620\n",
            "Ensemble - Accuracy: 0.7831, F1: 0.8057\n",
            "\n",
            "Fold 5/5\n",
            "Creating quantum-inspired features...\n",
            "Creating quantum-inspired features...\n",
            "random_forest - Accuracy: 0.8008, F1: 0.8146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9pgpLO08cL5O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}